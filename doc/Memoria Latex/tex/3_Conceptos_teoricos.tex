\capitulo{3}{Theoretical concepts}

In the following, all the theoretical concepts relevant for the understanding of the project will be explained.

\section{Decision Trees}

A decision tree ~\cite{decisiontrees} is a versatile supervised learning algorithm used for classification and regression tasks. Its goal is to predict the value of a variable based on previously processed input. Its hierarchical structure includes a root node and several internal and leaf nodes.

It starts at the root, which represents the feature that best separates the underlying dataset based on a certain criterion. An example would be information gain, which will be explained in a later section. From there branches extend to internal nodes, also called decision nodes. These internal nodes also represent features along with a decision rule that tells us how to further split the data. These features are continuously evaluated until homogenous subsets are created by the leaf nodes. These represent all the possible outcomes of the dataset with each one corresponding to a class label.

Decision tree learning utilizes a divide and conquer approach, iteratively finding the best split points until all or most of the input data is classified.

An example of a decision tree which evaluates whether a person should buy a game or not:
\imagen{decision_tree_example}{Decision tree example}{1}

\subsection{Subsecciones}

Además de secciones tenemos subsecciones.

\subsubsection{Subsubsecciones}

Y subsecciones. 

\section{Entropy} \label{entropy}
In the context of decision tree algorithms, entropy ~\cite{entropy_dash} can be viewed as a measure of impurity in a dataset. It can also be described as a measure of disorder, uncertainty or the expected surprise of a classification, but going forward, the term impurity will be used to avoid confusion. In datasets with binary classes, where variables can only have two possible outcome values, the entropy value lies between 0 and 1, inclusive. The higher the entropy, the more impure the dataset is. In a binary-class dataset, a node that has an equal distribution of, e.g., 5 instances belonging to one class and the other 5 instances belonging to the other class, would have an entropy value of 1. Inversely, a node that has all its instances belong to only one class would have an entropy value of 0, making´it a pure node.
The value is calculated using the following formula:


\[ E(X) = -\sum_{i=1}^{n} {p_i} {log_2 (p_i)} \]


$E(X)$ is the entropy of dataset $X$, $n$ describes the number of classes in the dataset, and $p_i$ the proportion of instances in class $i$ or, in other words, the probability of an instance belonging to class $i$.

This following graph ~\cite{entropy_T} that describes the entropy function in relation to the composition of a node shows really well how entropy is used to determine how impure a node is:
\imagen{entropy_graph}{Descriptive graph of the entropy function}{.7}
In a dataset with binary classes, the x-axis describes the amount of instances in a dataset belonging to the positive class while the y-axis measures their entropy values. It records the lowest values at the extremes where there are only or no positive instances recorded in a given dataset while the highest impurity is reached when the numbers of positive and negative instances are equal.
However, it must be mentioned that the value of entropy can reach values higher than 1 if the dataset possesses more than 2 class labels. 


\subsection{Conditional Entropy}
In the context of decision trees, the conditional entropy~\cite{conditional_entropy_wiki} $E(Y\mid X)$measures the uncertainty in the target variable $Y$, which is usually the class labels, given a certain value of the attribute $X$ that is used for splitting a node.
It can be described as the weighted sum of $E(Y\mid X = x)$ for every possible value $x$ of $X$:
\[E(Y\mid X) = \sum_{x \in X} {p(x)} {E(Y\mid X = x)}\]
The formula sums up the conditional entropies for all the possible values $x$ of attribute $X$. As the weight, it uses $p(x)$, which is the proportion between the number of instances that have the attribute value $x$ and the size of the dataset.
Calculating the conditional entropies for the individual values $x$ of attribute $X$ is possible with the following formula:
\[ E(Y\mid X = x) = -\sum_{y \in Y} {p(y\mid x)} {log_2 {(p(y\mid x))}} \]
$p(y\mid x)$ represents the proportion of instances in the dataset with attribute value $x$ that also have the class value $y$. It sums over all the possible values of the target variable $Y$. It can be used synonymous with the formula for $E(X)$ that was previously shown in \ref{entropy}

In the end, this conditional entropy calculation evaluates the effectiveness of splitting a node based on the particular attribute $X$. The lower the conditional entropy, the lower the impurity of the data after splitting the node based on the particular attribute the conditional entropy was calculated for.


\section{Machine Learning}
Machine learning~\cite{machine_learning} is a field of artificial intelligence which focuses on developing algorithms that are able to learn from given input data and, based on that learned knowledge, generalize to unseen data. Machine learning problems can be categorized into three main tasks: Classification problems, Regression problems, and Clustering problems.

\subsection{Classification problems} \label{classification_problems}
Classification problems~\cite{classification_regression} in machine learning describe the process of predicting a class or category of a given input. The labeled input data is learned with all its attributes and values to reach the goal of building a model that can also categorize new data, that has not been seen before, into one of the given classes or categories. An example would be classifying emails as spam or not spam. This type of machine learning problem will be discussed in more detail in the section \ref{classification}.

\subsection{Regression problems}
Regression problems~\cite{classification_regression} in machine learning describe the process of predicting a continuous value based on labeled input data. Instead of predicting a defined class or category, the goal here is to build a model that can predict the quantity of something. An example would be predicting the salary of a person based on their education degree and previous work experience.

\subsection{Clustering problems}
Clustering problems~\cite{clustering} in machine learning do not aim to predict something, like the previous two. Given some unlabeled input data, the goal is to group similar data points together based on certain features to generate an output of clusters inside of which the data of points are more similar to each other than to data points in other clusters.

\subsection{Decision Trees}
With decision trees~\cite{decisiontrees} ultimately following the goal to create a model that predicts the value of a target variable and doing so by learning labeled input data, they are used for both classification and regression tasks.


\section{Classification} \label{classification}
As mentioned before in \ref{classification_problems}, the goal in classification tasks~\cite{classification_regression} is to assign a class or a label to some input data. Classification aims to create a model that can also generalize to new data. To achieve this, there are multiple algorithms and evaluation metrics that are used to measure a model's performance.

\subsection{Classification algorithms}

\subsubsection{K-Nearest Neighbors}
K-Nearest Neighbors~\cite{knn}, or short KNN, is an algorithm that uses distance as a measure to determine K of a data point's nearest neighbors. Based on that information, a classification or prediction is made about which of the groups the data point is grouped with. KNN does not go through a training stage, instead it just stores and memorizes the training dataset.

\subsubsection{Support Vector Machine}
Support Vector Machine~\cite{svm}, or short SVM, is an algorithm that can perform linear as well as non-linear classification. Linear classifiers assume that a data set is linearly separable while non-linear classifiers are not bound to that restrictive belief. With SVM being able to perform both methods, it is able to find a hyperplane that best separates different classes in a data set by maximizing the margin between classes. 

\subsubsection{Logistic Regression}
This algorithm~\cite{logistic_regression} is mostly used for binary classification problems. It uses the logistic function to determine the probability of a given input belonging to a certain class.

\subsection{Evaluation metrics}

\subsubsection{Confusion Matrix}
A confusion matrix~\cite{evaluation_metrics} is a table that is used to describe a classification model's performance. It presents an overview of the predictions on the test data set against the actual classes:
\imagen{confusion_matrix}{Example of a confusion matrix}{.5}
Where:
TP = True Positive; TN = True Negative; FP = False Positive; FN = False Negative.
It is useful for measuring other metrics like Accuracy, Precision and Recall

\subsubsection{Accuracy}
Accuracy~\cite{evaluation_metrics} measures how many of all the classified instances were given the correct label.
It uses the following formula:
\[ Accuracy = {TP + TN}/{TP + TN + FP + FN} \]
This metric is useful for well balanced data sets. If 99 of the data set's instances belonged to one class and only 1 belonged to the other, likeliness is high that the used model would always predict the class with the higher amount of instances, resulting in a 99\% accuracy due to the data set's composure.

\subsubsection{Precision}
Precision~\cite{evaluation_metrics} describes the ratio of correctly predicted positive cases to the total predicted positive cases. In short words, it measures the accuracy of the positive class predictions and is calculated with the following formula:
\[ Precision = {TP}/{TP + FP} \]
Precision is useful for cases where False Positives are a bigger concern than False Negatives

\subsubsection{Recall (Sensitivity}
Recall~\cite{evaluation_metrics} describes how many of the actual positive cases our model was able to correctly predict. It measures the ability of the model to find all positive instances and is calculated using the following formula:
\[ Recall = {TP}/{TP + FN} \]
Recall is useful when False Negatives are of a higher concern than False Positives.


\section{Referencias}

Las referencias se incluyen en el texto usando cite~\cite{wiki:latex}. Para citar webs, artículos o libros~\cite{koza92}, si se desean citar más de uno en el mismo lugar~\cite{bortolot2005, koza92}.


\section{Imágenes}

Se pueden incluir imágenes con los comandos standard de \LaTeX, pero esta plantilla dispone de comandos propios como por ejemplo el siguiente:

\imagen{escudoInfor}{Autómata para una expresión vacía}{.5}



\section{Listas de items}

Existen tres posibilidades:

\begin{itemize}
	\item primer item.
	\item segundo item.
\end{itemize}

\begin{enumerate}
	\item primer item.
	\item segundo item.
\end{enumerate}

\begin{description}
	\item[Primer item] más información sobre el primer item.
	\item[Segundo item] más información sobre el segundo item.
\end{description}
	
\begin{itemize}
\item 
\end{itemize}

\section{Tablas}

Igualmente se pueden usar los comandos específicos de \LaTeX o bien usar alguno de los comandos de la plantilla.

\tablaSmall{Herramientas y tecnologías utilizadas en cada parte del proyecto}{l c c c c}{herramientasportipodeuso}
{ \multicolumn{1}{l}{Herramientas} & App AngularJS & API REST & BD & Memoria \\}{ 
HTML5 & X & & &\\
CSS3 & X & & &\\
BOOTSTRAP & X & & &\\
JavaScript & X & & &\\
AngularJS & X & & &\\
Bower & X & & &\\
PHP & & X & &\\
Karma + Jasmine & X & & &\\
Slim framework & & X & &\\
Idiorm & & X & &\\
Composer & & X & &\\
JSON & X & X & &\\
PhpStorm & X & X & &\\
MySQL & & & X &\\
PhpMyAdmin & & & X &\\
Git + BitBucket & X & X & X & X\\
Mik\TeX{} & & & & X\\
\TeX{}Maker & & & & X\\
Astah & & & & X\\
Balsamiq Mockups & X & & &\\
VersionOne & X & X & X & X\\
} 